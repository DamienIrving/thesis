
\chapter{A minimum standard for publishing computational results}\label{c:reproducibility}

%=========================================================================

\begin{synopsis}
This chapter explains the rationale behind the approach taken in documenting the computational aspects of the research and considers how it might be adopted as a minimum communication standard by relevant academic journals.
\vspace{1cm}
\end{synopsis}


%===========================

The analysis of ERA-Interim data presented in the preceding chapters required the use of numerous software packages. Most of these contain many more sub-packages / libraries, which together perform a specific task such as data visualisation, EOF analysis, spherical grid rotation or manipulation of netCDF file attributes. A major component of the research therefore involved writing (many thousands of lines of) code to link these packages and sub-packages into a coherent data analysis workflow. For parts of the workflow where no ready-made software package existed, it was also necessary to write a new package / function for that task (e.g. a new Hilbert transform function was written by combining pre-existing Fourier analysis routines from the SciPy library).

Given the computationally intensive nature of this data analysis, it would be impossible for another researcher to reproduce the results without details of the software and code. This is true of essentially all contemporary weather and climate science, and yet traditional academic publishing formats and conventions do not allow for the documentation of these aspects of the research methodology. As discussed in Section \ref{s:reproducibility_overview}, the absence of adequate software and code documentation, along with other issues such as access to research data, has led to something of a reproducibility crisis in modern computational research \citep[e.g.][]{Peng2011}. 

In order to ensure the reproducibility of the zonal wave and PSA pattern analyses presented in this thesis, a comprehensive procedure was devised for documenting the computational aspects of a research project. In the first instance, this involved a broad consultation of the literature to find out (a) why researchers do not currently publish their code, and (b) what computational best practices they should follow when they do. An approach was then designed (and implemented in Section \ref{s:computation} and the journal publications arising from the thesis) that sought to reduce the barriers for researchers, while at the same time promoting established best practices in scientific computing. In order to stimulate further discussion and progress in the area of reproducibility, this approach was then used as a starting point for defining a minimum communication standard that could be adopted by journals in the weather and climate sciences, as well as a set of lesson materials aimed at researchers wanting to learn the skills required to adhere to those standards. 

The details of the literature review (Section \ref{s:reproducibility_review}), rationale behind the procedure for documenting the computational aspects of a research project (Section \ref{s:reproducibility_approach}), proposed minimum communication standard (Section \ref{s:reproducibility_standards}) and lesson materials (Section \ref{s:reproducibility_lessons}) are documented in this chapter.


%===========================

\section{Literature review}\label{s:reproducibility_review}

The literature on the topic of barriers to publishing code is relatively sparse, however a recent survey of the machine learning community found that the top reason for not sharing code was the perceived time required to prepare it for publication, followed by the prospect of dealing with questions from users \citep{Stodden2010}. While many researchers are sympathetic to the ideals of open science and reproducible research, it appears that the practicalities seem too difficult and time consuming, particularly when the pressure to publish is so strong and unrelenting. The vast majority of survey respondents also identified as self-taught programmers, which suggests that the computational competency of the community is relatively low. 

While there are numerous scientific best practices documented in the literature \citep[e.g.][]{Wilson2014a}, only a few are directly relevant to the documentation of reproducible research. The first is that researchers should be saving the commands used to produce a particular result in a file (called a script), so that the sequence of commands can be repeated at a later time. While this best practice is something that most weather and climate scientists are comfortable with, it is important to note that authors should not aim to produce a single script for each key result. This is because a second relevant best practice is that of modularising code, rather than copying and pasting. Since duplication of any segment of code is error prone (because updates would need to be applied multiple times), researchers should instead aim to develop a whole library/repository of code containing many interconnected scripts (i.e. that each import key code segments from elsewhere). The revision history of that repository can then be tracked using a version control system like Git, Subversion or Mercurial, so that previous versions can be easily retrieved (e.g. to find out exactly what commands were used to produce a particular figure that was generated six months ago). These version control systems are easily linked to an online hosting service such as GitHub or Bitbucket, which is the means by which a code repository can be made publicly available.

Besides these core best practices, the literature points to a wide range of tools for assisting with reproducibility. While these tools are no doubt proposed with good intentions, the bewildering array of options is one reason why an individual working in the weather and climate sciences might place reproducibility in the `too hard' basket. An appropriate solution for any given scientist no doubt exists within that collection of tools, but it is heavily obscured. Consider the `regular' scientist described in Box \ref{box:regular_scientist}. In consulting the literature on reproducible computational research, they would be confronted with options including data provenance tracking systems like VisTrails \citep{Freire2012} and PyRDM \citep{Jacobs2014}, software environment managers like Docker and Vagrant \citep{Stodden2014}, and even online services like RunMyCode.org where your code and data can be run by others \citep{Stodden2012}. These might be good options for small teams of software engineers or experienced scientific programmers dealing with very large workflows (e.g. the post-processing of thousands of model runs), complex model simulations and/or production style code (e.g. they might be developing a satellite retrieval algorithm that has high re-use potential in the wider community), but a regular scientist has neither the requisite computational experience or a research problem of sufficient scale and complexity to necessarily require and/or make use of such tools. The procedure outlined below was designed with this regular scientist in mind. It looked to minimise both the time involved and the complexity of the associated tools, while at the same time remaining faithful to established best practices in scientific computing.


\begin{featurebox}

\begin{tcolorbox}[width=\textwidth]

The procedure for documenting computational results outlined in this chapter was developed with a `regular' weather/climate scientist in mind. A characterisation of this scientist is given below and is based on the few documented surveys of how computational scientists do their work \citep{Hannay2009,Stodden2010,Momcheva2015}, editorials describing current computational practices \citep[e.g.][]{Easterbrook2014} and the author's personal experience teaching at numerous Software Carpentry workshops over the past few years (Section \ref{s:reproducibility_lessons}). This regular scientist:
\begin{itemize}
\item Works with publicly available data (e.g. reanalysis data) that is often large in size (e.g. it might be tens or hundreds of gigabytes), but is not so large as to be considered `big data.' 
\item Acquired the knowledge to develop and use scientific software from peers and through self-study, as opposed to formal education and training.
\item Primarily relies on software like Python, MATLAB, IDL, NCL or R, which has a large user/support base and is relatively simple to install on a Windows, Mac or Linux computer.
\item Does most of their work on a desktop or intermediate computer (as opposed to a supercomputer).
\item Is only writing code for a specific task/paper and is not looking for community uptake.  
\item Works on their own or in a very small team (e.g. 2--3 other scientists) and does not have access to professional software developers for support.
\end{itemize}

Some scientists might be regular in most but not all aspects of their work (e.g. all the points above might apply except they occasionally use a highly specialised software package that does not have a large support base) so this characterisation can be thought of as a baseline or minimum level of computation that essentially all weather and climate scientists are engaged in.  

\end{tcolorbox}

\caption{\label{box:regular_scientist}
Description of a regular scientist.}
\end{featurebox}


%===========================  
  
\section{Documenting the computational aspects of a research project}\label{s:reproducibility_approach}

At first glance, the only difference between a regular thesis or journal article and that presented here is the addition of a short computation section (e.g. Section \ref{s:computation}). That section accompanies the traditional description of data and methods and briefly cites the major software packages used in the research, before pointing the reader to three key supplementary items: (1) a more detailed description of the software used, (2) a version controlled and publicly available code repository and (3) a collection of supplementary log files that capture the data processing steps taken in producing each key result. The repository was hosted at a popular code sharing website called GitHub, while the detailed software description and log files were hosted at Figshare \citep{IrvingFigshare2016}, which is a website where researchers commonly archive the `long tail' of their research (e.g. supplementary figures, code and data). 

\subsection{Software description}

There is an important difference between citing the software that was used in a study (so that authors get appropriate academic credit) and describing it in sufficient detail so as to convey the precise version and computing environment \citep{Jackson2012}. Recognising this, the computation section should begin with a high-level description of the software used, which includes citations to any papers written about the software. Authors of scientific software are increasingly publishing with journals like the \textit{Journal of Open Research Software}, so it is important for users of that software to cite those papers within their manuscripts. This high-level description is also useful for briefly articulating what general tasks each software item was used for (e.g. plotting, data analysis, file manipulation). Such an overview does not provide sufficient detail to recreate the computing environment used in the study, so it is also necessary to provide a link to a supplementary file that documents the precise version of each software package used and the operating system upon which it was run (namely the name, version number, release date, institution and DOI or URL; Box \ref{box:software_description}). 

\begin{featurebox}

\begin{tcolorbox}[width=\textwidth]

\begin{lstlisting}[basicstyle=\footnotesize\ttfamily, breaklines=true]
Operating system: 

Ubuntu. 12.04. April 2012. Canonical Ltd.
    http://www.ubuntu.com/

Software packages:

netCDF Operators. 4.4.6. September 2014. netCDF Operators Project.
    http://sourceforge.net/projects/nco/
Climate Data Operators. 1.7.0. October 2015. Max Plank Institut fur Meteorologie. Hamburg, Germany. 
    https://code.zmaw.de/projects/cdo
Anaconda. 2.0.1. July 2014. Continuum Analytics. Austin, Texas.
    http://docs.continuum.io/anaconda/
Python. 2.7.10. May 2015. Python Software Foundation.
    https://www.python.org/
NumPy. 1.10.1. October 2015. NumPy Developers. 
    http://www.numpy.org/
SciPy. 0.16.0. July 2015. SciPy Developers.
    http://www.scipy.org/scipylib/
xray. 0.5.1. June 2015. xray Developers.
    http://xray.readthedocs.org/
windspharm. 1.3.1. June 2014. Andrew Dawson.
    http://ajdawson.github.io/windspharm/
eofs. 0.5.0. June 2014. Andrew Dawson.
    http://ajdawson.github.io/eofs/
pyqt_fit. 1.3.3. August 2014. Pierre Barbier de Reuille. 
    https://pyqt-fit.readthedocs.org/
matplotlib. 1.4.3. July 2015. Matplotlib Development Team.
    http://matplotlib.org/
seaborn. 0.6.0. June 2015. Michael Waskom.
    http://stanford.edu/~mwaskom/software/seaborn/
iris. 1.8.1. June 2015. Met Office. Exeter, England.
    http://scitools.org.uk/
cartopy. 0.13.0. June 2015. Met Office. Exeter, England.
    http://scitools.org.uk/
\end{lstlisting}

\end{tcolorbox}

\caption{\label{box:software_description}
Software environment used in producing the results presented in this thesis \citep[and documented at][]{IrvingFigshare2016}.}

\end{featurebox}  

\subsection{Code repository}

Most of the examples of reproducible research currently available in the literature produce a pristine GitHub or Bitbucket repository for their paper (i.e. one that only contains code directly relevant to that paper), however this should not be an expectation for regular scientists who are not looking for broad-scale community uptake of their code. Not only is this a time consuming practice that would likely involve a degree of cutting and pasting, it also goes against the workflow that version control promotes. By providing a link to their everyday repository instead, authors can quickly and easily signal to readers what code was used to produce the results in their paper. The authors may further develop that code in the future, so the reader then also has easy access to the latest version if they would like to use it in their own work and/or suggest improvements (referred to as a `pull request' in version control parlance). There are all sorts of extra bits and pieces of code in the everyday repository that is linked to in Section \ref{s:computation}. Readers will probably never look at that code (because it is not referred to in the associated log files), but what is the harm if they do? Students and other scientists could potentially learn from it, and in the best case scenario they would inform the author of a bug or potential improvement to the code.        

In addition to providing a link to the associated Github repository, this thesis provides \citep[on Figshare;][]{IrvingFigshare2016} a static snapshot of the repository taken at the time of publication. The motivation for doing this was to ensure that a static version of the code is available in case the associated GitHub repository is ever moved or deleted. Unlike GitHub, archiving sites such as Figshare and Zenodo issue DOIs, which function as a perpetual link to the resource and can only be obtained by agencies that commit to maintain a reliable level of preservation \citep{Potter2015}. Since this snapshot does not provide a revision history of the code, it would not have been appropriate to provide it \textit{instead} of the link to GitHub. Not all of the results presented in a research paper or thesis will necessarily have been generated with the very latest version of the code, hence the need for the revision history (and even if they were generated with the latest version, it is not possible to submit a pull request to Figshare or Zenodo). Both Figshare and Zenodo provide a simple interface for importing code directly from GitHub, so the process is very straightforward.

\subsection{Log files}\label{s:log_files}

A code repository and software description on their own are not much use to a reader; they also need to know how that code was used in generating the results presented. It turns out that in the weather and climate sciences, the answer to adequately documenting the computational steps involved in producing a given result has been staring us in the face for years. As a community we have almost universally adopted a self-describing file format (i.e. a format where metadata can be stored within the file) called network Common Data Form (netCDF), which means we have been able to develop numerous software tools for processing and manipulating data stored in that format. The most well known of these are a collection of command line tools known as the NetCDF Operators (NCO) and Climate Data Operators (CDO). Whenever an NCO or CDO command is executed, a time stamp followed by a copy of the command line entry is automatically placed into the global attributes of the output netCDF file, thus maintaining a history of the provenance of that data (see Box \ref{box:log_file} for examples of NCO and CDO entries).

What is important here is not the specific software tools or file format (there are many researchers who do not use NCO, CDO, or netCDF files) but rather the deceptively simple method for recording previous computational steps. Using any of the programming languages common to the weather and climate sciences, a user can obtain details of the associated command line entry and append such text to the global attributes of a netCDF file (or a corresponding metadata text file if dealing with file formats that are not self-describing). In fact, in all of these languages such tasks can be achieved with just a few lines of additional code. This thesis provides a log file containing a complete NCO/CDO-style history for each figure; see Box \ref{box:log_file} for details of one of those files and the associated page on Figshare \citep{IrvingFigshare2016} for the complete set.

An important feature of these log files is that they are both readable and writable by any weather and climate scientist. If advanced practitioners are tracking their computational steps with tools like VisTrails then they can certainly submit log files (or equivalent documentation) exported from those tools, but as a minimum standard it is important that elaborate tools are not a requirement. By spelling out every single computational step (i.e. from initial data download to the final plot/result), the log files also ensure that readers do not need to be familiar with build tools like Make or other workflow management tools in order to figure out which computational steps were executed and in what order. Other features of note include:
\begin{itemize}
\item Besides a slight amendment to the initial download entry of the log file shown in Box \ref{box:log_file} (the default text provided by the ERA-Interim data server was not particularly self explanatory), no manual editing of its contents was done. This means that if a reviewer asked for a slight modification to the figure, for instance, the regeneration of a new log file would be trivial. By resisting the urge to clean up the file (e.g. one might consider removing path details) it also doubles as a record that is highly useful to the author in retracing their own steps (e.g. they could use it to recall where they stored the output data on their local machine).
\item As previously mentioned, it cannot be assumed that the latest version of any code repository was used to generate all the results in a given paper. The unique version control revision number (known as a hash value) was therefore recorded in the log files, wherever a script written by the author was executed. Languages like Python, R and MATLAB are able to link with version control systems like Git, so the retrieval of the revision number can be automated.
\item When more than one input file is passed to an NCO or CDO function, the history of only one of those files is retained in the output file. On occasions where this is not appropriate (i.e. where the histories of the multiple input files are very different), it is important to ensure that the history of all input files is retained. There are a number of examples of this in the log files provided on Figshare. 
\end{itemize}
  
 
 \begin{featurebox}

\begin{tcolorbox}[width=\textwidth]

\begin{lstlisting}[basicstyle=\footnotesize\ttfamily, breaklines=true]
Mon Nov 16 09:11:28 2015: /usr/local/anaconda/bin/python /home/STUDENT/dbirving/climate-analysis/visualisation/plot_hilbert.py /mnt/meteo0/data/simmonds/dbirving/ERAInterim/data/va_ERAInterim_500hPa_030day-runmean_native.nc va /mnt/meteo0/data/simmonds/dbirving/ERAInterim/data/zw/figures/hilbert_zw_w19_va_ERAInterim_500hPa_030day-runmean_native-55S_1986-05-22_2006-07-29.eps 1 2 --latitude -55 -55 --dates 1986-05-22 2006-07-29 --wavenumbers 1 9 --figure_size 15 6 --envelope 1 9 (Git hash: a9573e4)

Tue Jun 30 07:35:49 2015: cdo runmean,30 /mnt/meteo0/data/simmonds/dbirving/ERAInterim/data/va_ERAInterim_500hPa_daily_native.nc /mnt/meteo0/data/simmonds/dbirving/ERAInterim/data/va_ERAInterim_500hPa_030day-runmean_native.nc

Wed Mar 18 17:17:14 2015: cdo mergetime va_ERAInterim_500hPa_daily-2014-06-01-to-2014-12-31_native.nc ../data/va_ERAInterim_500hPa_daily_native_orig.nc ../data/va_ERAInterim_500hPa_daily_native.nc

Mon Nov 10 17:15:49 2014: ncatted -O -a level,va,o,c,500hPa va_ERAInterim_500hPa_daily_native.nc

Mon Nov 10 16:31:05 2014: ncatted -O -a long_name,va,o,c,northward_wind va_ERAInterim_500hPa_daily_native.nc

Thu Aug 21 10:57:46 2014: ncatted -O -a axis,time,c,c,T va_ERAInterim_500hPa_daily_native.nc

Thu Aug 21 10:34:46 2014: ncrename -O -v v,va va_ERAInterim_500hPa_daily_native.nc

Thu Aug 21 10:26:49 2014: cdo invertlat -sellonlatbox,0,359.9,-90,90 -daymean va_ERAInterim_500hPa_6hourly_native.nc va_ERAInterim_500hPa_daily_native.nc

Thu Aug 21 10:14:59 2014: cdo mergetime ../download/va_ERAInterim_500hPa_6hourly-1979-1988_native_unpacked.nc ../download/va_ERAInterim_500hPa_6hourly-1989-1998_native_unpacked.nc ../download/va_ERAInterim_500hPa_6hourly-1999-2008_native_unpacked.nc ../download/va_ERAInterim_500hPa_6hourly-2009-2014_native_unpacked.nc va_ERAInterim_500hPa_6hourly_native.nc

Thu Aug 21 10:13:35 2014: ncpdq -P upk va_ERAInterim_500hPa_6hourly-2009-2014_native.nc va_ERAInterim_500hPa_6hourly-2009-2014_native_unpacked.nc

Wed Aug 20 23:16:22 2014: Initial download of 6 hourly, 500 hPa meridional wind ERA-Interim data in 5 or 10 year chunks (e.g. va_ERAInterim_500hPa_6hourly-2009-2014_native.nc) from http://apps.ecmwf.int/datasets/data/interim-full-daily/
\end{lstlisting}

\end{tcolorbox}

\caption{\label{box:log_file}
Log file corresponding to Figure \ref{fig:example_hilbert}. Details regarding the software and code referred to in the log file are provided in Section \ref{s:computation}.}

\end{featurebox}  
 
 
%===========================

\section{A formal minimum standard}\label{s:reproducibility_standards}

\subsection{Implications and practicalities}

Before formally proposing a minimum standard for the communication of computational results, it is worth considering the implications and practicalities of adopting a standard based on the approach described above. The reproducibility of published results would presumably improve, which may also lead to increased trust, interest and citations \citep{Piwowar2007}, but what else would it mean for authors and reviewers? Would there be barriers to overcome in implementing such a standard, and what could be done to make the transition easier?

\subsubsection{Authors}

As previously mentioned, \citet{Stodden2010} identified the time involved as the largest barrier to sharing code. There are numerous authors who argue that the best practices like scripting and version control save time in the long run \citep[e.g.][]{Sandve2013,Wilson2014a} and objective evidence is beginning to emerge in support of this claim \citep{Simperler2015}. This means that once researchers have learned and adopted these practices, they may actually save time. In the author's experience as a Software Carpentry instructor (Section \ref{s:reproducibility_lessons}), many weather and climate scientists are comfortable with the idea of scripting, but very few use version control. Learning these new skills is not overly time consuming (Software Carpentry teaches them in a short two-day workshop), but on a local level it requires an individual or institution to take the lead in liaising with Software Carpentry to find volunteer instructors and to coordinate other logistics. A good example is the Australian Meteorological and Oceanographic Society (AMOS), which has hosted a Software Carpentry workshop alongside its annual conference for the past three years running. Of course, it is also possible to learn these skills by following an online tutorial (e.g. all the Software Carpentry lessons are available online), but there is an added benefit to the social aspect of a workshop. It helps to reduce the embarrassment many scientists have about the quality of their code (i.e. they see that their peers are no `better' at coding than they are), which is an important part of achieving the required cultural shift towards an acceptance of code sharing \citep{Barnes2010}.

The other potentially time consuming task associated with adopting a minimum standard would be dealing with requests for assistance. One suggested solution to this problem is to make it clear that authors are not obliged to support others in repeating their computations \citep{Easterbrook2014}. This is probably the only feasible solution, but it is worth noting that even if not formally obliged, some authors may fear that refusing requests will make it look like they are uncooperative and/or have something to hide. 

Some researchers also face barriers relating to security and proprietary, particularly if they are using large code bases that have been developed for research and/or operations within government laboratories, national weather bureaus and private companies \citep{Stodden2010}. Such code bases are increasingly being made public (e.g. the Australian Bureau of Meteorology and CSIRO host the code for their Climate and Weather Science Laboratory in a public GitHub repository), but any proposed minimum standard would need to allow some flexibility for researchers who are unable to make their code public for these reasons (the code archived by MPI-M is available via request only, which might be an acceptable solution in many cases). For those concerned about getting appropriate academic credit for highly novel and original code, a separate publication (e.g. in the \textit{Journal of Open Research Software}) or software license \citep[e.g.][]{Morin2012} might also be an option.

\subsubsection{Reviewers}

In requiring authors to make their code available, it would be important to convey to reviewers that they are not expected to review the code associated with a submission; they simply have to check that it is sufficiently documented (i.e. that the code is available in an online repository and that log files have been provided for all figures and key results). Not only would it be unrealistic to have reviewers examine submitted code due to the wide variety of software tools and programming languages out there, it would also be inconsistent with the way scientific methods have always been reviewed. For instance, in the 1980s it was common for weather and climate scientists to manually identify weather systems of interest (e.g. polar lows) from satellite imagery. The reviewers of the day were not required to go through all the satellite images and check that the author had counted correctly, they simply had to check that the criteria for polar low identification was adequately documented. This is not to say that counting errors were not made on the part of authors (as with computer code today there were surely numerous errors/bugs), it was just not the job of the reviewer to find them. Author errors are revealed when other studies show conflicting results and/or when other authors try to replicate key results, which is a process that would be greatly enhanced by having a minimum standard for the communication of computational results. This idea of conceptualising the peer review of code as a post publication process is consistent with the publication system envisaged by the open evaluation movement \citep[e.g.][]{Kriegeskorte2012}. 
  
\subsection{Proposed standards}

To assist in establishing a minimum standard for the communication of computational results, it is proposed that the following text (or something in the same spirit) could be inserted into the author and reviewer guidelines of journals in the weather and climate sciences (institutions that have their own internal review process could also adopt these guidelines). In places the language borrows from the guidelines recently adopted by \textit{Nature} \citep{Nature2014}. It is anticipated that a journal would provide links to examples of well documented computational results to help both authors and reviewers in complying with these guidelines. The journal could decide to host the supplementary materials itself (i.e. the software description, log files and static snapshot code repository), or encourage the author to host these items at an external location that can guarantee persistent, long-term access (e.g. an institutionally supported site like MPI-M provides for its researchers or an online academic archive such as Figshare or Zenodo).

\subsubsection{Author guidelines}

If computer code is central to any of the paper's major conclusions, then the following is required as a minimum standard: 
\begin{enumerate}
\item A statement describing whether (and where) that code is available and setting out any restrictions on accessibility. 
\item A high-level description of the software used to execute that code (including citations for any academic papers written to describe that software).
\item A supplementary file outlining the precise version of the software packages and operating system used. This information should be presented in the following format: name, version number, release date, institution, DOI or URL.
\item A supplementary log file for each major result (including key figures) listing all computational steps taken from the initial download/attainment of the data to the final result (i.e. the log files describe how the code and software were used to produce the major results). 
\end{enumerate}

It is recommended that items 1 and 2 are included in a `Computation' (or similarly named) section within the manuscript itself. Any practical issues preventing code sharing will be evaluated by the editors, who reserve the right to decline a paper if important code is unavailable. While not a compulsory requirement, best practice for code sharing involves managing code with a version control system such as Git, Subversion or Mercurial, which is then linked to a publicly accessible online repository such as GitHub or Bitbucket. In the log files a unique revision number (or hash value) can then be quoted to indicate the precise version of the code repository that was used. Authors are not expected to produce a brand new repository to accompany their paper; an `everyday' repository which also contains code not relevant to the paper is acceptable. Authors should also note that they are not obliged to support reviewers or readers in repeating their computations.

\subsubsection{Reviewer guidelines}

The reviewer guidelines for most journals already ask if the methodology is explained in sufficient detail so that the paper's scientific conclusions could be tested by others. Such guidelines could simply be added to as follows: `If computer code is central to any of those conclusions, then reviewers should ensure that the authors are compliant with the minimum standards outlined in the author guidelines. It should be noted that reviewers are not obliged to assess or execute the code associated with a submission. They must simply check that it is adequately documented.'   


%===========================  

\section{Software Carpentry lesson materials}\label{s:reproducibility_lessons}

Software Carpentry is a volunteer organisation whose members teach basic programming skills to researchers in science, engineering and medicine \citep{Wilson2014}. Over the past few years it has run hundreds of two-day workshops at research institutions around the world and is now a global leader in computing education. One key to the success of the organisation has been the fact that most qualified Software Carpentry instructors are research scientists as opposed to professional software developers. This avoids a well-known teaching phenomenon called expert blind spot; the instructors can remember what it is like not to understand basic programming concepts and are able to write scientist-appropriate lesson materials.

As part of the 2013 AMOS conference in Melbourne, Greg Wilson (the founder of Software Carpentry) flew to Australia to run a workshop for the AMOS community. Like all Software Carpentry workshops, it delivered lessons on automating tasks using the Unix shell, structured programming in Python (including how to write programs that are executable at the command line) and version control using Git. While these are all the basic skills required to adhere to the proposed communication standards, it was felt that an additional `capstone' lesson was required to pull all those skills together into a coherent example specific to the weather and climate sciences. Over the next few months the author completed the Software Carpentry instructor training course and developed this capstone lesson \citep{IrvingSWC2015}, before delivering it and the rest of the core Software Carpentry materials at a workshop alongside the 2014 AMOS conference in Hobart (and again at the 2015 conference in Brisbane). Importantly, that lesson describes how to create the log files discussed in Section \ref{s:log_files}. 


%===========================

\section{Discussion}

In order to combat the reproducibility crisis in published computational research, a simple procedure for communicating computational results has been demonstrated and its rationale discussed. The procedure involves authors providing three key supplementary items: (1) a description of the software packages and operating system used, (2) a (preferably version controlled and publicly accessible) code repository, and (3) a collection of supplementary log files that capture the data processing steps taken in producing each key result. It should provide a starting point for weather and climate scientists (and perhaps computational scientists more generally) looking to publish reproducible research, and could be adopted as a minimum standard by relevant academic journals.

The procedure/standard was developed to be consistent with recommended computational best practices and seeks to minimise the time burden on authors, which has been identified as the most important barrier to publishing code. In particular, best practice dictates that at a minimum weather and climate scientists should be (a) writing data analysis scripts so they can re-run their analyses, (b) using version control to manage those scripts for backup and ease of sharing/collaboration and (c) storing the details of their analysis steps in the global history attribute of their netCDF data files (or following an equivalent process for other file formats) to ensure the complete provenance of their data. In order to make their published results reproducible, it follows that the minimum an author would need to do is simply make those history attributes available (via log files) along with the associated code repository and a description of the software used to execute that code. The attainment of this minimum standard would involve a slight change to the workflow of many regular weather and climate scientists (e.g. most do not use version control), however the standard has been designed to only require skills that can be learned very quickly (e.g. at a two-day Software Carpentry workshop).  

While widespread adoption of this minimum standard would be a great starting point for reproducible research, it is worth noting that as a community we should ultimately aim much higher. By way of analogy, minimum standards in the construction industry ensure that buildings will not fall over or otherwise kill their inhabitants, but if everyone only built to those minimum standards our cities would be hugely energy inefficient. The proposed minimum standard for computational research ensures that published results are reproducible (which is a big improvement on the current state of affairs), but recreating workflows from the log files, daily code repositories and software descriptions of even just moderately complex analyses would be a tedious and time consuming process. Once comfortable with the skills and processes required to meet the minimum standard, authors should seek to go beyond them to improve the comprehensibility of their published computational results, in the same way that builders should strive for a five-star energy rating. The precise tools and methods used in this endeavour will vary from author to author; basic analyses might only require the inclusion of informative REAMDE files that explain in plain language how to execute the code, while others might choose to provide informative flow diagrams exported from provenance tracking systems like VisTrails, pre-package their code/software for ease of installation (e.g. for inclusion in the Python Package Index) and/or make their software environment available via Docker. As previously mentioned, it would not be appropriate to include these many varied (and often complex) options in any minimum standards, but they represent an excellent next-step for scientists who have mastered the basics and will hopefully see more uptake as the computational competency of the community improves over time.

  







